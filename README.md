# 🧠📈 Industrial Steel Defect Detection Using XGBoost

This project walks through a full machine learning workflow to classify types of defects in industrial steel plates. It covers everything from data preprocessing to model selection, hyperparameter tuning, and predictions. The final model, XGBoost with SMOTE resampling, achieved strong performance across multiple metrics and is able to distinguish well between all defect types.

📌 Originally built for a technical screening and later adapted into a portfolio project.

## 🎯 Goal

Build a robust machine learning model to accurately classify defects in industrial steel plates. This includes data cleaning, feature exploration, model comparison, hyperparameter tuning, and testing model behavior on synthetic data to ensure generalization.

## 🏭 Real-World Relevance

In steel manufacturing, early and accurate defect detection is critical to reduce production downtime, material waste, and customer returns. 

- **False negatives** or missed defects may let faulty materials reach customers which is costly and reputation-damaging.
- **False positives** or flagging non-defective plates increase manual inspection time and slow production.

This model can serve as a foundational component in an automated inspection pipeline to minimize both failure risks and operational inefficiencies.

## 🗂️ Project Structure

```plaintext
Steel-fault-classifier/
│
├── data/
│   ├── raw/                               # Original dataset
│   ├── processed/                         # Cleaned dataset
│   └── feature-engineering/               # Data after feature engineering
│
├── figures/                               # Visualizations
│   ├── fault-type-distribution.png        
│   └── xgboost-model-confusion-matrix.png                 
│
├── notebooks/                             # Jupyter notebooks
│   └──steel-fault-classifier.ipynb        # End-to-end project
│
├── models/                                # Trained model
│   ├── xgboost_model.json                 # Final model (JSON)
│   └── xgboost_model.pkl                  # Final model (Pickle)
│
├── results/                               # Model output
│   └── metrics                            # Model metrics
│       └── xgboost-model-metrics.txt
│
├── requirements.txt                       # Dependencies
└── README.md                              # Project documentation  
```

## 📘 Project Overview

1. **Introduction** – Problem overview and motivation
2. **Dataset** – Overview of the data used
3. **Data Cleaning** – Prepare the data for analysis
4. **Exploratory Data Analysis** – Understand distributions and correlations
5. **Feature Engineering** – Create new features to improve modeling
6. **Preprocessing** – Standardize and scale the features
7. **Modeling** – Train and evaluate multiple classification models
8. **XGBoost Optimization** – Hyperparameter tuning and SMOTE resampling
9. **Robustness Testing** – Test model robustness using synthetic data
10. **Conclusions** – Final thoughts and model performance summary

## 💻 Installation

Install all dependencies with:

```bash
pip install -r requirements.txt
```

## ▶️ Running the Project

1. Clone the repo

   ```bash
   git clone https://github.com/herrerovir/Steel-fault-classifier.git
   ```
2. Move into the directory

   ```bash
   cd Steel-fault-classifier
   ```
3. Open the notebook

   ```bash
   jupyter notebook
   ```

## 📊 Dataset

The dataset contains 1941 steel plate samples with 27 features and 7 binary labels indicating different defect types. It is sourced from Kaggle’s [Faulty Steel Plates Dataset](https://www.kaggle.com/datasets/uciml/faulty-steel-plates).

## 🧠 Modeling

Multiple classification algorithms were tested, including:

- Decision Tree
- Random Forest
- XGBoost
- Support Vector Machine
- Multilayer Perceptron

XGBoost was selected as the best-performing model. To address class imbalance, **SMOTE** was applied to synthetically augment minority class samples and improve detection of less frequent defect types. Hyperparameter tuning via grid search further optimized model performance. The final XGBoost model demonstrates strong precision and recall across all defect classes.

## 🔮 Robustness Testing with Synthetic Data

To evaluate model robustness beyond the original dataset, synthetic samples were generated by sampling from the statistical distributions of the training features. This approach simulates new, unseen examples to test model stability and predictive consistency. Results showed the model maintained expected class distribution patterns, especially for frequent defect classes like class 6 (the most common defect type), confirming reliable generalization.

## 🚀 Future Work

- Deploy the model as a **Streamlit app** to allow interactive defect prediction
- Add **SHAP explainability** for model transparency and trust