# ğŸ§ ğŸ“ˆ Industrial Steel Defect Detection Using XGBoost

This project walks through a full machine learning workflow to classify types of defects in industrial steel plates. It covers everything from data preprocessing to model selection, hyperparameter tuning, and predictions. The final model, XGBoost with SMOTE resampling, achieved strong performance across multiple metrics and is able to distinguish well between all defect types.

ğŸ“Œ Originally built for a technical screening and later adapted into a portfolio project.

## ğŸ¯ Goal

Build a robust machine learning model to accurately classify defects in industrial steel plates. This project focuses on cleaning the data, exploring key features, and tuning the best algorithm to deliver precise and reliable predictions that help improve quality control in manufacturing.

## ğŸ—‚ï¸ Project Structure

```plaintext
Steel-fault-classifier/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                               # Original dataset
â”‚   â”œâ”€â”€ processed/                         # Cleaned dataset
â”‚   â””â”€â”€ feature-engineering/               # Data after feature engineering
â”‚
â”œâ”€â”€ figures/                               # Visualizations
â”‚   â”œâ”€â”€ fault-type-distribution.png        
â”‚   â””â”€â”€ xgboost-model-confusion-matrix.png                 
â”‚
â”œâ”€â”€ notebooks/                             # Jupyter notebooks
â”‚   â””â”€â”€steel-fault-classifier.ipynb        # End-to-end project
â”‚
â”œâ”€â”€ models/                                # Trained model
â”‚   â”œâ”€â”€ xgboost_model.json                 # Final model (JSON)
â”‚   â””â”€â”€ xgboost_model.pkl                  # Final model (Pickle)
â”‚
â”œâ”€â”€ results/                               # Model output
â”‚   â””â”€â”€ metrics                            # Model metrics
â”‚       â””â”€â”€ xgboost-model-metrics.txt
â”‚
â”œâ”€â”€ requirements.txt                       # Dependencies
â””â”€â”€ README.md                              # Project documentation  
```

## ğŸ“˜ Project Overview

1. **Introduction** â€“ Problem overview and motivation
2. **Dataset** â€“ Overview of the data used
3. **Data Cleaning** â€“ Prepare the data for analysis
4. **Exploratory Data Analysis** â€“ Understand distributions and correlations
5. **Feature Engineering** â€“ Create new features to improve modeling
6. **Preprocessing** â€“ Standardize and scale the features
7. **Modeling** â€“ Train and evaluate multiple classification models
8. **XGBoost Optimization** â€“ Hyperparameter tuning and SMOTE resampling
9. **Forecasting** â€“ Generate synthetic data to test predictions
10. **Conclusions** â€“ Final thoughts and model performance summary

## ğŸ’» Installation

Install all dependencies with:

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Running the Project

1. Clone the repo

   ```bash
   git clone https://github.com/herrerovir/Steel-fault-classifier.git
   ```
2. Move into the directory

   ```bash
   cd Steel-fault-classifier
   ```
3. Open the notebook

   ```bash
   jupyter notebook
   ```

## ğŸ“Š Dataset

The dataset was sourced from [Kaggle](https://www.kaggle.com/datasets/uciml/faulty-steel-plates).
It contains 1,941 rows and 34 columns, with 27 features and 7 binary labels indicating different types of steel defects.

## ğŸ§  Modeling

The goal was to find the best algorithm to classify steel defects. Models tested:

- Decision Tree
- Random Forest
- XGBoost
- Support Vector Machine
- Multilayer Perceptron

After comparison, **XGBoost** showed the best performance. The model was further tuned using a grid search and improved with **SMOTE** to balance class distributions. The final model demonstrated strong performance in both precision and recall.

## ğŸ”® Forecasting

To simulate predictions on new data, synthetic samples were generated by sampling from the distribution of the training features. Although this does not perfectly reflect real-world scenarios, it allows testing how the model behaves on unseen data.

ğŸ“ˆ The model predicted class distributions similar to its known patterns, particularly for frequent defects like class `6`. This aligns with expectations and confirms the model's consistency beyond the test set.

## ğŸ“ˆ Results

The final XGBoost model, optimized through hyperparameter tuning and improved with SMOTE resampling, demonstrated strong classification performance across all defect types, including minority classes. Evaluation metrics such as F1-score, ROC AUC, learning curves, and precision-recall curves confirmed its robustness and generalization capability. To simulate deployment, the model was also tested on synthetic unseen data, where it maintained consistent predictive behavior. Overall, the model proved effective at identifying and differentiating steel plate defects, making it a solid solution for industrial quality control.
