{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Classification\n",
    "\n",
    "## Steel Plates Anomaly Detection\n",
    "\n",
    "**Author:** Virginia Herrero [Email](mailto:v.herrero@outlook.com) | [LinkedIn](https://www.linkedin.com/in/virginia-herrero-casero/) | [GitHub](https://github.com/herrerovir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "Este proyecto consiste en un proceso completo de machine learning con el objetivo de predecir el tipo de defecto o anomalía producido en placas de acero industrial. Este notebook ofrece una guía paso a paso de cada etapa del ejercicio, desde el preprocesamiento de datos hasta la evaluación del modelo, destacando las técnicas y metodologías utilizadas a lo largo del camino.\n",
    "\n",
    "Elegí este conjunto de datos por su relevancia en la industria 2.0 y en el control de calidad. La detección de anomalías en productos industriales es un excelente ejemplo para mostrar mis habilidades en ingeniería y ciencia de datos. Este proyecto ilustra cómo los enfoques basados en datos pueden optimizar la gestión de calidad y la toma de decisiones, siendo aplicables a diversas industrias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "El conjunto de datos utilizado en este proyecto se obtuvo de la página web Kaggle [here](https://www.kaggle.com/datasets/uciml/faulty-steel-plates).\n",
    "\n",
    "Las variables de este conjunto de datos son las siguientes:\n",
    "\n",
    "- `X_Minimum`\n",
    "\n",
    "- `X_Maximum`\n",
    "\n",
    "- `Y_Minimum`\n",
    "\n",
    "- `Y_Maximum`\n",
    "\n",
    "- `Pixels_Areas`\n",
    "\n",
    "- `X_Perimeter`\n",
    "\n",
    "- `Y_Perimeter`\n",
    "\n",
    "- `Sum_of_Luminosity`\n",
    "\n",
    "- `Minimum_of_Luminosity`\n",
    "\n",
    "- `Maximum_of_Luminosity`\n",
    "\n",
    "- `Length_of_Conveyer`\n",
    "\n",
    "- `TypeOfSteel_A300`\n",
    "\n",
    "- `TypeOfSteel_A400`\n",
    "\n",
    "- `Steel_Plate_Thickness`\n",
    "\n",
    "- `Edges_Index`\n",
    "\n",
    "- `Empty_Index`\n",
    "\n",
    "- `Square_Index`\n",
    "\n",
    "- `Outside_X_Index`\n",
    "\n",
    "- `Edges_X_Index`\n",
    "\n",
    "- `Edges_Y_Index`\n",
    "\n",
    "- `Outside_Global_Index`\n",
    "\n",
    "- `LogOfAreas`\n",
    "\n",
    "- `Log_X_Index`\n",
    "\n",
    "- `Log_Y_Index`\n",
    "\n",
    "- `Orientation_Index`\n",
    "\n",
    "- `Luminosity_Index`\n",
    "\n",
    "- `SigmoidOfAreas`\n",
    "\n",
    "Los 7 defectos en las placas de acero son los siguientes: \n",
    "\n",
    "- `Pastry`\n",
    "\n",
    "- `Z_Scratch`\n",
    "\n",
    "- `K_Scatch`\n",
    "\n",
    "- `Stains`\n",
    "\n",
    "- `Dirtiness`\n",
    "\n",
    "- `Bumps`\n",
    "\n",
    "- `Other_Faults`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga el archivo CSV **steel-plates-faults-dataset** como un DataFrame de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Normalizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modeling and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Hypertuning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Re-sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Saving the model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Steel-plates-faults-dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Limpia y preprocesa el conjunto de datos antes de seguir con el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Renombrar columnas**\n",
    "\n",
    "Se renombraron algunas columnas para mejorar la legibilidad y la comprensión del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\"TypeOfSteel_A300\" : \"Steel_Type_A300\",\n",
    "                          \"TypeOfSteel_A400\" : \"Steel_Type_A400\",\n",
    "                          \"LogOfAreas\" : \"Log_of_Areas\",\n",
    "                          \"SigmoidOfAreas\" : \"Sigmoid_of_Areas\",\n",
    "                          \"K_Scatch\" : \"K_Scratch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tipos de datos**\n",
    "\n",
    "Verifica que todas las columnas tengan los tipos de datos apropiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Valores nulos**\n",
    "\n",
    "Identifica y elimina cualquier valor nulo en el conjunto de datos cuando sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total of null values in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay valores nulos en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Valores duplicados**\n",
    "\n",
    "Verifica si hay valores duplicados en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay ninguna entrada duplicada en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Outliers**\n",
    "\n",
    "Revisa el resumen estadístico del conjunto de datos para detectar posibles valores atípicos. Esta evaluación inicial permitirá identificar cualquier valor inusual que necesite un análisis más detallado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes float format to display two decimals\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primera vista, columnas como **X_Minimum**, **X_Maximum**, **Y_Minimum**, **Y_Maximum**, **Pixel_Areas**, **X_Perimeter**, **Y_Perimeter**, **Sum_of_Luminosity** y **Steel_plate_thickness** pueden contener valores atípicos. Esta conclusión se basa en la observación de que los valores máximos superan tanto la media, la mediana, y el tercerl cuartil, lo que puede indicar la presencia de outliers. Dado que estos pueden ser claros indicadores de defectos en las placas de acero, he decidido mantener los valores atípicos para no reducir las observaciones del conjunto de datos y contribuir a una mejor predicción del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **El conjunto de datos limpio:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_detection = df.copy()\n",
    "defect_detection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "defect_detection.to_csv(\"Steel-plates-faults-cleaned-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "En esta sección, se realiza un análisis exploratorio de datos en profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Target Variable**\n",
    "\n",
    "El primer paso es examinar la variable objetivo para obtener una visión general de su distribución.\n",
    "\n",
    "El objetivo de este proyecto es predecir fallos en las placas de acero. Este conjunto de datos contiene 7 columnas que representan los 7 posibles fallos. Estas 7 columnas son la variable objetivo de este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of all target columns on the datasest\n",
    "fig, ax = plt.subplots(2, 4, figsize = (20, 8))\n",
    "ax = ax.flatten()\n",
    "for i, col in enumerate(defect_detection.columns[-7:]):\n",
    "    sns.histplot(defect_detection[col], ax = ax[i], color = \"#74add1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica si hay observaciones sin fallos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of fault columns\n",
    "faults = [\"Pastry\", \"Z_Scratch\", \"K_Scratch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"]\n",
    "\n",
    "# Get the observations with no faults\n",
    "no_faults = defect_detection[defect_detection[faults].sum(axis = 1) == 0]\n",
    "\n",
    "# Display the results\n",
    "if no_faults.empty:\n",
    "    print(\"There are no observations with no faults.\")\n",
    "else:\n",
    "    print(\"Number of observations with no faults:\")\n",
    "    print(no_faults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De este análisis rápido se puede concluir que no hay observaciones sin fallos. Ahora verifica si hay fallos que ocurren simultáneamente en una misma observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum defect values per row\n",
    "defect_detection[faults].sum(axis = 1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se confirma que todas las observaciones contienen solo un tipo de fallo. Por lo tanto, este problema se mantiene como una clasificación multicategórica en lugar de una clasificación multietiqueta. Lo siguiente será echar un vistazlo a la distribución de todos los fallos para tener una visión clara de cuántas observaciones por fallo hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_faults = defect_detection[faults].sum().sort_values(ascending=True)\n",
    "sum_faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de tipos de fallos en el acero muestra que el fallo más común es el de **other faults**, seguido por **bumps** y **k_scratch**. El fallo menos frecuente en el conjunto de datos es **dirtiness** (suciedad).\n",
    "\n",
    "El conjunto de datos presenta un claro desequilibrio, ya que el número de instancias entre las diferentes clases de fallos varía ampliamente. Durante la etapa de modelado, será necesario abordar este problema. Algunas técnicas utilizadas para manejar conjuntos de datos desequilibrados incluyen: re-muestreo, el uso de algoritmos robustos contra el desequilibrio de clases, o aplicar métricas que consideren la distribución de clases, como el F1-score y la curva de precisión-recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_types_distribution_graph = plt.figure(figsize = (10, 4))\n",
    "ax = sns.barplot(sum_faults, color = \"#74add1\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "plt.xlabel(\"Tipo de defecto\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribución de los tipos de defectos en las placas de acero\", size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_types_distribution_pie = plt.figure(figsize = (11, 5))\n",
    "colors = [\"#d73027\", \"#f46d43\", \"#fdae61\", \"#fee090\", \"#abd9e9\", \"#74add1\", \"#4575b4\"]\n",
    "plt.pie(sum_faults, labels = sum_faults.index, startangle = 90, autopct = \"%1.0f%%\", shadow = True, colors = colors)\n",
    "plt.axis(\"equal\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribución de los tipos de defectos en las placas de acero\", pad = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Features**\n",
    "\n",
    "El siguiente paso es examinar cada característica del conjunto de datos para obtener una visión general de sus distribuciones. Dado que todas están en diferentes escalas, será necesario escalar las características en los próximos pasos.\n",
    "\n",
    "Los histogramas revelan información valiosa sobre la distribución de las características.\n",
    "\n",
    "- **Distribución normal**: Maximum_of_Luminosity, Empty_Index, Square_Index, Luminosity_Index, Minimum_of_Luminosity y Orientation_Index.\n",
    "- **Distribución sesgada**: Y_Minimum, Y_Maximum, Pixels_Areas, X_Perimeter, Y_Perimeter, Sum_of_Luminosity, Log_Y_Index y Edges_X_Index.\n",
    "- **Distribución uniforme**: X_Minimum, X_Maximum, EdgesIndex, Edges_Y_Index y SigmoidOfAreas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of all features on the datasest\n",
    "fig, ax = plt.subplots(9, 3, figsize = (18, 32))\n",
    "for i, col in enumerate(defect_detection.columns[:27]):\n",
    "    sns.histplot(defect_detection[col], ax = ax[i//3][i%3], color = \"#74add1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matriz de correlación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = defect_detection.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_heatmap_graph = plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(correlations, linewidths = 0.5, cmap = \"RdYlBu\")\n",
    "plt.title(\"Correlation Heatmap\", size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de correlación muestra las relaciones entre variables: el color rojo indica que la relación es negativa, el azul que la relación es positiva y el color amarillo que la relación es baja o nula. Puesto que hay un alto número de características correlacionadas es necesario crear nuevas características a partir de éstas para reducir el número total de características en el dataset y facilitar el modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "De acuerdo con los resultados obtenidos de la matrix de correlación, se crean nuevas variables. Las variables que ya no sean necesarias se eliminarán del conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables\n",
    "defect_detection[\"X_Range\"] = defect_detection[\"X_Maximum\"] - defect_detection[\"X_Minimum\"]\n",
    "defect_detection[\"Y_Range\"] = defect_detection[\"Y_Maximum\"] - defect_detection[\"Y_Minimum\"]\n",
    "defect_detection[\"Defect_Area\"] = (defect_detection[\"X_Perimeter\"] * defect_detection[\"Y_Perimeter\"])\n",
    "defect_detection[\"Luminosity_Range\"] = defect_detection[\"Maximum_of_Luminosity\"] - defect_detection[\"Minimum_of_Luminosity\"]\n",
    "defect_detection[\"Edge\"] = defect_detection[\"Edges_Index\"] / (defect_detection[\"Edges_X_Index\"] * defect_detection[\"Edges_Y_Index\"])\n",
    "defect_detection[\"Outside_X_Range\"] = defect_detection[\"Outside_X_Index\"] * defect_detection[\"X_Range\"]\n",
    "defect_detection[\"Log_Area\"] = defect_detection[\"Log_of_Areas\"] / (0.000001 + defect_detection[\"Log_X_Index\"] * defect_detection [\"Log_Y_Index\"])\n",
    "defect_detection[\"Luminosity_Sum_Range\"] = defect_detection[\"Sum_of_Luminosity\"] * defect_detection[\"Luminosity_Range\"]\n",
    "defect_detection[\"Log_Area_Sigmoid\"] = defect_detection[\"Log_Area\"] * defect_detection[\"Sigmoid_of_Areas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_detection.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"X_Minimum\", \"X_Maximum\", \"Y_Minimum\", \"Y_Maximum\", \"X_Perimeter\", \"Y_Perimeter\", \"Minimum_of_Luminosity\", \"Maximum_of_Luminosity\",\n",
    "                   \"Outside_X_Index\", \"Edges_Index\", \"Edges_X_Index\", \"Edges_Y_Index\",\"Log_of_Areas\", \"Log_X_Index\", \"Log_Y_Index\", \"Sum_of_Luminosity\", \n",
    "                   \"Luminosity_Range\", \"Sigmoid_of_Areas\"]\n",
    "\n",
    "defect_detection = defect_detection.drop(columns_to_drop, axis = 1)\n",
    "defect_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns after dropping the unnecessary ones\n",
    "defect_detection.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot again the correlation matrix with the new features\n",
    "correlations_2 = defect_detection.corr()\n",
    "correlations_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_heatmap_graph_2 = plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(correlations_2, linewidths = 0.5, cmap = \"RdYlBu\")\n",
    "plt.title(\"Correlation Heatmap\", size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Antes de comenzar con el modelado, será necesario hacer unas transformaciones en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Nueva columna de Fallos**\n",
    "\n",
    "Crea una nueva columna que contenga todos los tipos de fallos. Esta columna será la variable objetivo y facilitará el modelado y la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of faults and their corresponding values\n",
    "fault_mapping = {\"Pastry\": 0, \"Z_Scratch\": 1, \"K_Scratch\": 2, \"Stains\": 3, \"Dirtiness\": 4, \"Bumps\": 5, \"Other_Faults\": 6}\n",
    "\n",
    "# Initialize the Faults column\n",
    "defect_detection[\"Faults\"] = 0\n",
    "\n",
    "# Loop through each fault and assign the corresponding value\n",
    "for fault, value in fault_mapping.items():\n",
    "    defect_detection.loc[defect_detection[fault] == 1, \"Faults\"] = value\n",
    "\n",
    "# Display the first few rows\n",
    "defect_detection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop individual fault columns\n",
    "defect_detection.drop(faults, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_detection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dividir los datos**\n",
    "\n",
    "Al dividir los datos en un conjunto de entrenamiento y uno de prueba, se asegura que el escalado se base únicamente en los datos de entrenamiento, evitando que cualquier información del conjunto de prueba se filtren en el modelo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent features\n",
    "X = defect_detection.drop(\"Faults\", axis = 1)\n",
    "\n",
    "# Dependent or target variable\n",
    "y = defect_detection[\"Faults\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(\"Training set - X_train shape:\", X_train.shape)\n",
    "print(\"Testing set - X_test shape:\", X_test.shape)\n",
    "print(\"Training set - y_train shape:\", y_train.shape)\n",
    "print(\"Testing set - y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Escalado de características**\n",
    "\n",
    "El método utilizado es el de escalado estándar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado\n",
    "\n",
    "El objetivo de este ejercicio es construir un modelo que clasifique los defectos de placas de acero industrial. Para ello, el primer paso es encontrar el algoritmo que mejor rendimiento presente y a partir de ahí, se afinarán los hiperparámetros del este modelo para encontrar la mejor versión del modelo ganador. Los algoritmos elegidos para comenzar el modelado son: **Decision Tree**, **Random Forest**, **XGBoost**, **Support Vector Machine** y **Multilayer Perceptron**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decision Tree\n",
    "print(\"Decision Tree Model:\")\n",
    "dt_model = DecisionTreeClassifier(random_state = 42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "dt_predictions = dt_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, dt_predictions))\n",
    "dt_cm = confusion_matrix(y_test, dt_predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = dt_cm).plot(cmap = \"PuBu\")\n",
    "plt.title(\"Matriz de Confusión del modelo Decision Tree\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor real\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"Random Forest Model:\")\n",
    "rf_model = RandomForestClassifier(random_state = 42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "rf_cm = confusion_matrix(y_test, rf_predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = rf_cm).plot(cmap = \"PuBu\")\n",
    "plt.title(\"Matriz de Confusión del modelo Random Forest\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor real\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"XGBoost Model:\")\n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, xgb_predictions))\n",
    "xgb_cm = confusion_matrix(y_test, xgb_predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = xgb_cm).plot(cmap = \"PuBu\")\n",
    "plt.title(\"Matriz de Confusión del modelo XGBoost\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor real\")\n",
    "\n",
    "# 4. Support Vector Machine\n",
    "print(\"Support Vector Machine Model:\")\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_predictions = svm_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "svm_cm = confusion_matrix(y_test, svm_predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = svm_cm).plot(cmap = \"PuBu\")\n",
    "plt.title(\"Matriz de Confusión del modelo Support Vector Machine\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor real\")\n",
    "\n",
    "# 5. Multilayer Perceptron\n",
    "print(\"Multilayer Perceptron Model:\")\n",
    "mlp_model = MLPClassifier(random_state = 42, max_iter = 500)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "mlp_predictions = mlp_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, mlp_predictions))\n",
    "mlp_cm = confusion_matrix(y_test, mlp_predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = mlp_cm).plot(cmap = \"PuBu\")\n",
    "plt.title(\"Matriz de Confusión del modelo Multilayer Perceptron\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre todos los modelos probados, **XGBoost** destaca como el más efectivo, alcanzando una precisión de **0.79** y sobresaliendo tanto en precisión como en recall en varias clases. Su F1-score macro promedio de **0.78** refuerza su rendimiento equilibrado.\n",
    "\n",
    "**Random Forest** y **Multilayer Perceptron** le siguen de cerca con una precisión de **0.76**. Ambos ofrecen resultados sólidos, pero no logran igualar la efectividad de XGBoost. Por otro lado, **Árbol de Decisión** y **Máquina de Vectores de Soporte** solo alcanzan una precisión de **0.70**, teniendo más dificultades en la clasificación de algunas clases.\n",
    "\n",
    "**XGBoost** es la opción más acertada si el objetivo es maximizar la precisión del model y tener un rendimiento equilibrado. Además, un ajuste adicional de los hiperparámetros podría llevar su rendimiento a otro nivel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost es el modelo seleccionado para abordar esta tarea de clasificación. Ahora es el momento de comprender el modelo y encontrar los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning**\n",
    "\n",
    "Primero, se optimiza el modelo para mejorar su rendimiento. Este modelo ajustado funciona mejor que el original, pero aún presenta dificultades para clasificar las clases minoritarias, como la clase 4, que corresponde al defecto \"suciedad\" y tiene muy pocas observaciones en el conjunto de datos. Para solucionar este problema, es necesario aplicar una técnica de muestreo. Dado que las observaciones son escasas, decidí utilizar la técnica **SMOTE** para mejorar la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = xgb.XGBClassifier(random_state = 42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\"max_depth\": [3, 5, 7],\n",
    "              \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "              \"n_estimators\": [100, 200, 300],\n",
    "              \"subsample\": [0.6, 0.8, 1.0]}\n",
    "\n",
    "# Set up Grid Search\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = param_grid,\n",
    "                           scoring = \"f1_macro\",\n",
    "                           cv = 3,\n",
    "                           verbose = 1,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SMOTE**\n",
    "\n",
    "SMOTE es una técnica de re-muestreo que crea nuevas muestras ajustando ligeramente los datos existentes hacia sus vecinos. Funciona eligiendo aleatoriamente una muestra de la clase minoritaria, identificando sus k vecinos más cercanos y generando nuevos puntos de datos al escalar la distancia hacia esos vecinos. De esta manera, se mantiene la integridad de la clase minoritaria y se enriquece el conjunto de datos.\n",
    "\n",
    "Una vez dividido el conjunto de datos en entrenamiento y prueba, aplicamos SMOTE **solamente** al conjunto de entrenamiento. Esto asegura que el conjunto de prueba siga siendo una representación fiel de la distribución original de los datos y previene cualquier filtración de información desde el conjunto de entrenamiento. Así, garantizamos que el proceso de evaluación sea más sólido y confiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Fit the XGBoost model on the resampled data\n",
    "xgb_smote_model = XGBClassifier(random_state = 42)\n",
    "xgb_smote_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "xgb_smote_pred = xgb_smote_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, xgb_smote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo XGBoost con SMOTE se destaca claramente como el mejor en comparación con el modelo XGBoost ajustado, logrando una precisión del 80% y un F1-score macro promedio de 0.80. Aunque la precisión de ambos modelos es similar, uno de los puntos fuertes de este modelo es la capacidad para identificar eficazmente instancias de clases minoritarias, como la clase 4, donde logró un valor de recall del 88%, mientras que el modelo original alcanzó 62% y el ajustado solo alcanzó el 50%.\n",
    "\n",
    "Además, el modelo SMOTE muestra un rendimiento constante en las clases mayoritarias, lo que demuestra su fiabilidad y un rendimiento bastante sólido en general. Esto lo convierte en la mejor opción para aplicaciones del mundo real, especialmente en conjuntos de datos desbalanceados donde cada clase es importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open(\"xgboost_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(xgb_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Virginia\\Python\\VHC-TestDataScience-1\\model', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matriz de Confusión**\n",
    "\n",
    "Analizar la matriz de confusión nos ayuda a comprender las clasificaciones incorrectas y a identificar áreas potenciales de mejora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot XGBoost Confusion Matrix\n",
    "xgb_smote_model_cm = confusion_matrix(y_test, xgb_predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = xgb_smote_model_cm).plot(cmap = \"PuBu\")\n",
    "plt.title(\"Matriz de confusión del modelo XGBoost\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importancia de las características**\n",
    "\n",
    "Identificar las características clave que impulsan las predicciones es fundamental para comprender cómo el modelo toma decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgboost_feature_importance = plt.figure(figsize = (15, 3))\n",
    "xgb.plot_importance(xgb_smote_model,\n",
    "                    importance_type = \"weight\",\n",
    "                    color = \"#74add1\",\n",
    "                    title = \"Importancia de características de XGBoost\",\n",
    "                    xlabel = \"Peso\",\n",
    "                    ylabel = \"Características\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ROC AUC**\n",
    "\n",
    "Evalúa el rendimiento del modelo de manera integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_bin = label_binarize(y_test, classes = [0, 1, 2, 3, 4, 5, 6])\n",
    "y_score = xgb_smote_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Compute ROC AUC for each class\n",
    "roc_auc = roc_auc_score(y_bin, y_score, average = \"macro\")\n",
    "print(f\"XGBoost Model ROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_bin = label_binarize(y_test, classes = [0, 1, 2, 3, 4, 5, 6])\n",
    "n_classes = y_bin.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], xgb_smote_model.predict_proba(X_test_scaled)[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot the curves\n",
    "plt.figure(figsize = (11, 4))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], color = colors[i], label = \"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Tasa de Falsos Positivos\")\n",
    "plt.ylabel(\"Tasa de Verdaderos Positivos\")\n",
    "plt.title(\"Curva Característica de Operación del Receptor (ROC)\")\n",
    "plt.legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Curvas de Aprendizaje**\n",
    "\n",
    "Evalúa el comportamiento del entrenamiento del modelo y detecta problemas de sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves of the model\n",
    "plt.figure(figsize = (11, 4))\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(xgb_smote_model, X_resampled, y_resampled, cv = 5, n_jobs = -1, train_sizes = np.linspace(0.1, 1.0, 10))\n",
    "train_scores_mean = train_scores.mean(axis = 1)\n",
    "test_scores_mean = test_scores.mean(axis = 1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, color = \"#74add1\", label = \"Puntuación de entrenamiento\")\n",
    "plt.plot(train_sizes, test_scores_mean, color = \"#f46d43\", label = \"Puntuación de validación cruzada\")\n",
    "plt.title(\"Curva de aprendizaje del modelo XGBoost\")\n",
    "plt.xlabel(\"Tamaño de entrenamiento\")\n",
    "plt.ylabel(\"Puntuación\")\n",
    "plt.legend(loc = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Curva de Precisión-Recall**\n",
    "\n",
    "Evalúa el rendimiento del modelo en términos de precisión y recuperación, especialmente útil en casos de clases desbalanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(11, 4))\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_bin[:, i], xgb_smote_model.predict_proba(X_test_scaled)[:, i])\n",
    "    plt.plot(recall, precision, color = colors[i], label = f\"Curva de Precisión-Recuperación de la clase {i}\")\n",
    "plt.xlabel(\"Recuperación\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.title(\"Curva de Precisión-Recuperación\")\n",
    "plt.legend(loc = \"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "El objetivo de este ejercicio fue construir un modelo de machine learning para detectar anomalías o defectos en placas de acero industrial. Para lograrlo, se inició con la limpieza del conjunto de datos, un paso fundamental para asegurar la integridad y confiabilidad de la información. Seguidamente, se realizó un análisis exploratorio detallado que permitió comprender mejor la distribución y las relaciones entre las variables. Este análisis incluyó la evaluación individual de cada variable y la exploración de interacciones significativas entre pares de ellas. Esto permitió crear nuevas variables que hacen más fácil el modelado. \n",
    "\n",
    "Para determinar el modelo óptimo para la predicción de anomalías, se probaron varios enfoques: decision tree, random forest, xgboost, support vector machine y multilayer perceptron. Se concluyó que el modelo con mejor rendimiento fue XGBoost. Posteriormente, se buscó encontrar los hiperparámetros ideales para este modelo. Dado que durante la exploración de datos se identificó un desbalance en el conjunto, se aplicó la técnica de re-muestreo **SMOTE**. Este modelo no solo mostró el mejor rendimiento, sino que también destacó en precisión y recall.\n",
    "\n",
    "En resumen, se construyó un modelo de machine learning robusto capaz de distinguir entre clases minoritarias y mayoritarias, ofreciendo un rendimiento sólido y constituyendo la mejor opción para resolver este problema.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
